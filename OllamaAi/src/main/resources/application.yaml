server:
  port: 8085
spring:
  ai:
    ollama:
      chat:
        model: llama2
  application:
    name: OLLAMA-AI
  config:
    import: configserver:${CONFIG_SERVER_URL:http://localhost:9296}
    
    
